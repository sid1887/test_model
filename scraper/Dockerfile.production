# Multi-stage Dockerfile for Scraper Service Production
FROM node:18-alpine as builder

# Set working directory
WORKDIR /app

# Install build dependencies
RUN apk add --no-cache \
    python3 \
    make \
    g++ \
    git

# Copy package files
COPY package*.json ./

# Install all dependencies (including dev dependencies for building)
RUN npm ci

# Copy source code
COPY . .

# Run any build steps if needed (e.g., TypeScript compilation)
# RUN npm run build

# Remove dev dependencies
RUN npm prune --only=production

# Stage 2: Production runtime
FROM node:18-alpine as production

# Create app user for security
RUN addgroup -g 1001 -S nodejs && \
    adduser -S scraper -u 1001

# Install runtime dependencies for Puppeteer
RUN apk add --no-cache \
    chromium \
    nss \
    freetype \
    freetype-dev \
    harfbuzz \
    ca-certificates \
    ttf-freefont \
    curl \
    dumb-init

# Tell Puppeteer to skip installing Chromium
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true \
    PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser

# Set working directory
WORKDIR /app

# Copy production dependencies from builder
COPY --from=builder --chown=scraper:nodejs /app/node_modules ./node_modules

# Copy application code
COPY --chown=scraper:nodejs package*.json ./
COPY --chown=scraper:nodejs src/ ./src/
COPY --chown=scraper:nodejs start.js ./

# Create logs directory with proper permissions
RUN mkdir -p /app/logs && chown scraper:nodejs /app/logs

# Switch to non-root user
USER scraper

# Expose port
EXPOSE 3001

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:3001/health || exit 1

# Use dumb-init to handle signals properly
ENTRYPOINT ["dumb-init", "--"]

# Start the application
CMD ["node", "start.js"]
